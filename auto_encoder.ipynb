{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import corpora, models, similarities, downloader\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a skip connection auto-encoder architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_shape, compression_size):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # assuming the embeddings are of dimensions 300 and each sentence has 10 words/tokens\n",
    "\n",
    "        embedding_dimension = 300\n",
    "        num_of_words = 10\n",
    "\n",
    "        mlp_input_size = int(embedding_dimension * num_of_words)\n",
    "\n",
    "        self.layer1 = nn.Linear(mlp_input_size, 2400)\n",
    "        self.layer2 = nn.Linear(2400, 1500)\n",
    "        self.layer3 = nn.Linear(1500, compression_size)\n",
    "        \n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        #################################################### \n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "\n",
    "    def forward(self, features):\n",
    "\n",
    "        features_flat = torch.flatten(features, start_dim=1)\n",
    "        \n",
    "        #################################################### \n",
    "\n",
    "        out1 = self.layer1(features_flat)\n",
    "        out1 = self.relu(out1)\n",
    "        \n",
    "        out2 = self.layer2(out1)\n",
    "        out2 = self.relu(out2)\n",
    "\n",
    "        out3 = self.layer3(out2)\n",
    "    \n",
    "        #################################################### \n",
    "\n",
    "        return out3, out2, out1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        embedding_dimension = 300\n",
    "        num_of_words = 10\n",
    "\n",
    "        mlp_output_size = int(embedding_dimension * num_of_words)\n",
    "\n",
    "        self.layer1 = nn.Linear(input_size, 1500)\n",
    "        self.layer2 = nn.Linear(1500, 2400)\n",
    "        self.layer3 = nn.Linear(2400, mlp_output_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "\n",
    "        # Pass through the layers\n",
    "        out1 = self.layer1(x1)\n",
    "        out1 = self.relu(out1)\n",
    "\n",
    "        out2 = self.layer2(out1 + x2)\n",
    "        out2 = self.relu(out2)\n",
    "\n",
    "        out3 = self.layer3(out2 + x3)\n",
    "\n",
    "        # Reshaping the final output into a 300-dimensional embedding for 10 words\n",
    "        batch_size = out3.size(0)\n",
    "        out = out3.view(batch_size, 10, 300)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_shape, compression_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_shape\n",
    "\n",
    "        self.encoder = Encoder(input_shape, compression_size)\n",
    "        self.decoder = Decoder(compression_size, input_shape)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, features):\n",
    "\n",
    "        # implementing variational auto-encoder.\n",
    "        \n",
    "        out1 = self.encoder(features)\n",
    "        out2 = self.relu(out1)\n",
    "        # in order to implement variational auto-encoders, the forward pass should return the mean and the variance as well.\n",
    "        # this will in turn be passed to the loss function which uses them to calculate the kl divergence.\n",
    "        out2 = self.decoder(out2)\n",
    "\n",
    "        # out1 is the output for the encoder, out2 is the output for the decoder.\n",
    "        \n",
    "        return out2, out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_training(model, loss_function, optimizer, train_data, n_epochs, update_interval):\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    for n in range(n_epochs):\n",
    "        for i, image in enumerate(tqdm(train_data)):\n",
    "\n",
    "            # pre-processing training data so that we can directly feed it to the model.\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ex_pred = model(image)\n",
    "\n",
    "            ex_label = image\n",
    "\n",
    "            loss = loss_function(ex_pred, ex_label)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            ##############################################################\n",
    "        \n",
    "            if i % update_interval == 0:\n",
    "                losses.append(round(loss.item(), 2)) # This will append your losses for plotting -- please use \"loss\" as the name for your loss\n",
    "        \n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing the dataset so that it can be fed to the model.\n",
    "\n",
    "model = Autoencoder(3000, 900) \n",
    "loss_function = nn.MSELoss()                        \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr) \n",
    "\n",
    "model, losses = autoencoder_training(model, loss_function, optimizer, train_data, n_epochs, update_interval)\n",
    "\n",
    "# testing and validation later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to Consider:\n",
    "\n",
    "-) Pre-processing the dataset so that we have converted the texts into a numpy array of M words times N dimensions based on which word2vector training model we are trying to use - utilize word_embedding.ipynb .\n",
    "\n",
    "-) Once we have our trained model, we can pass the encodings of different layers into the appropriate decoding layers and plot how the loss compares relative to the complete process.\n",
    "\n",
    "-) Finally , once we have trained our model, defining a function that takes our reshaped output words * dimensions vector embedding and converts it into the most similar words using word2vector pre-defined methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
