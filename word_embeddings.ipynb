{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Word Embedding Models"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": 328,
>>>>>>> bdf7b1a (similar_words to work with word2vec)
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jeremysaccount/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
<<<<<<< HEAD
=======
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> bdf7b1a (similar_words to work with word2vec)
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import corpora, models, similarities, downloader\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading word2vec model\n",
      "Downloading gigaword model\n",
      "Downloading fasttext model\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading word2vec model\")\n",
    "word2vec_model = gensim.downloader.load('word2vec-google-news-300')\n",
    "print(\"Downloading gigaword model\")\n",
    "glove_model = gensim.downloader.load('glove-wiki-gigaword-100')\n",
    "print(\"Downloading fasttext model\")\n",
    "fasttext_model = gensim.downloader.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
=======
   "execution_count": 330,
>>>>>>> bdf7b1a (similar_words to work with word2vec)
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s\\d]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    # print(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordToVector(word, model):\n",
    "    try:\n",
    "        return model[word]\n",
    "    except KeyError:\n",
    "        # Handle out-of-vocabulary words\n",
    "        return np.zeros(model.vector_size)  # Return zero vector for OOV words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textToVectors(text, model):\n",
    "    tokens = tokenize(text)  # Tokenize the text\n",
    "    vectors = [wordToVector(token, model) for token in tokens]  # Convert words to vectors\n",
    "    vectors = np.array(vectors)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
=======
   "execution_count": 333,
>>>>>>> bdf7b1a (similar_words to work with word2vec)
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_vectors = textToVectors(\"The quick brown fox jumped. Over the lazy, brown Dog!\", word2vec_model)\n",
    "glove_vectors = textToVectors(\"The quick brown fox jumped. Over the lazy, brown Dog!\", glove_model)\n",
    "fasttext_vectors = textToVectors(\"The quick brown fox jumped. Over the lazy, brown Dog!\", fasttext_model)\n",
    "\n",
    "# print(word2vec_vectors)\n",
    "# print(glove_vectors)\n",
    "# print(fasttext_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Compression with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compressVectorsPCA(data, target_count):\n",
    "    pca = PCA(n_components=target_count)\n",
    "    compressed_data = pca.fit_transform(data.T).T\n",
    "    return compressed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 300)\n",
      "(7, 300)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(word2vec_vectors))\n",
    "if len(word2vec_vectors.shape) == 1:\n",
    "    word2vec_vectors = np.expand_dims(word2vec_vectors, axis=0)\n",
    "compressed_word2vec_vectors = compressVectorsPCA(word2vec_vectors, 7)\n",
    "print(np.shape(compressed_word2vec_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector To Best Match Word(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY WORKS FOR GLOVE MODEL\n",
    "\n",
    "def find_similar_words(target_vector, model, num_words=5):\n",
    "    # Compute cosine similarity between target vector and all word vectors\n",
    "    similarity_scores = np.dot(model.vectors, target_vector) / (np.linalg.norm(model.vectors, axis=1) * np.linalg.norm(target_vector))\n",
    "\n",
    "    # Get indices of top N words with highest similarity scores\n",
    "    top_indices = similarity_scores.argsort()[-num_words:][::-1]\n",
    "\n",
    "    # Get top N words and their similarity scores\n",
    "    similar_words = [(model.index_to_key[idx], similarity_scores[idx]) for idx in top_indices]\n",
    "\n",
    "    return similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_words(target_vector, model, num_words=5):\n",
    "    # Check if model is a full Word2Vec model\n",
    "    if hasattr(model, 'wv'):\n",
    "        vectors = model.wv.vectors\n",
    "        index_to_key = model.wv.index_to_key\n",
    "    # If not, assume it's a KeyedVectors object\n",
    "    else:\n",
    "        vectors = model.vectors\n",
    "        index_to_key = model.index_to_key\n",
    "\n",
    "    # Compute cosine similarity between target vector and all word vectors\n",
    "    similarity_scores = np.dot(vectors, target_vector) / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(target_vector))\n",
    "\n",
    "    # Get indices of top N words with highest similarity scores\n",
    "    top_indices = similarity_scores.argsort()[-num_words:][::-1]\n",
    "\n",
    "    # Get top N words and their similarity scores\n",
    "    similar_words = [(index_to_key[idx], similarity_scores[idx]) for idx in top_indices]\n",
    "\n",
    "    return similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best matches for random vector:  [('jedinak', 0.43938040144584456), ('non-compact', 0.41023222746997984), ('carapace', 0.38854165648895045), ('32-inch', 0.385508369375139), ('uninflated', 0.3851501959774927)]\n",
      "Best matches for specific vector for fast:  [('fast', 0.99999994), ('slow', 0.79597306), ('faster', 0.75118226), ('pace', 0.7462931), ('speed', 0.71333927)]\n"
     ]
    }
   ],
   "source": [
    "target_vector_random = np.random.rand(100)  # Random target vector\n",
    "similar_words_random = find_similar_words(target_vector_random, glove_model, num_words=5)\n",
    "\n",
    "target_word = \"fast\"\n",
    "target_vector_word = wordToVector(target_word, word)\n",
    "similar_words_word = find_similar_words(target_vector_word, word, num_words=5)\n",
    "\n",
    "print(\"Best matches for random vector: \", similar_words_random)\n",
    "print(f\"Best matches for specific vector for {target_word}: \", similar_words_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best matches for random vector:  [('wheelchair', 0.3978489827744443), ('fenced', 0.3821059002261432), ('half-mile', 0.38110007509520877), ('parked', 0.3794584821498503), ('all-weather', 0.3729744441735971)]\n",
      "Best matches for specific vector for fast:  [('fast', 1.0000001), ('quick', 0.5701605), ('rapidly', 0.5525554), ('Fast', 0.54902226), ('quickly', 0.53937227)]\n"
     ]
    }
   ],
   "source": [
    "target_vector_random = np.random.rand(100)  # Random target vector\n",
    "similar_words_random = find_similar_words(target_vector_random, glove_model, num_words=5)\n",
    "\n",
    "target_word = \"fast\"\n",
    "target_vector_word = wordToVector(target_word, word2vec_model)\n",
    "similar_words_word = find_similar_words(target_vector_word, word2vec_model, num_words=5)\n",
    "\n",
    "print(\"Best matches for random vector: \", similar_words_random)\n",
    "print(f\"Best matches for specific vector for {target_word}: \", similar_words_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "compressed_vector_1 = np.random.rand(7, 300)\n",
    "print('vector1\\n', compressed_vector_1)\n",
    "\n",
    "compressed_vector_2 = np.random.rand(7, 300)\n",
    "print('vector2\\n', compressed_vector_2)\n",
    "\n",
    "# Loss calculation\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "compressed_vector_1 = torch.from_numpy(compressed_vector_1).float()\n",
    "compressed_vector_2 = torch.from_numpy(compressed_vector_2).float()\n",
    "\n",
    "\n",
    "def cosine_similarity_loss(compressed_vectors_1, compressed_vectors_2):\n",
    "    # Normalize the compressed vectors\n",
    "    compressed_vectors_1 = F.normalize(compressed_vectors_1, dim=1)\n",
    "    compressed_vectors_2 = F.normalize(compressed_vectors_2, dim=1)\n",
    "    \n",
    "    # Calculate the cosine similarity between the compressed vectors\n",
    "    cosine_similarities = F.cosine_similarity(compressed_vectors_1, compressed_vectors_2)\n",
    "    \n",
    "    # Define the target labels (1 for similar pairs)\n",
    "    target = torch.ones_like(cosine_similarities)\n",
    "    \n",
    "    # Calculate the MSE loss\n",
    "    loss = F.mse_loss(cosine_similarities, target)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "loss = cosine_similarity_loss(compressed_vector_1, compressed_vector_2)\n",
    "print('loss', loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Define some similar and dissimilar sentences\n",
    "similar_sentences = [\"The cat sat on the mat\", \"A cat is sitting on the mat\"]\n",
    "dissimilar_sentences = [\"The cat sat on the mat\", \"Apple is a fruit\"]\n",
    "\n",
    "# Similar sentences are about the same sport (soccer)\n",
    "similar_sentences = [\"The soccer team won the match\", \"The football squad was victorious in the game\"]\n",
    "dissimilar_sentences = [\"The soccer team won the match\", \"The basketball player scored a three-pointer\"]\n",
    "\n",
    "# Similar sentences are about the same cooking action (baking a cake)\n",
    "similar_sentences = [\"She is baking a cake for the party\", \"A cake is being baked for the celebration\"]\n",
    "dissimilar_sentences = [\"She is baking a cake for the party\", \"He is frying an egg for breakfast\"]\n",
    "\n",
    "\n",
    "# Tokenize the sentences\n",
    "similar_sentences = [tokenize(sentence) for sentence in similar_sentences]\n",
    "dissimilar_sentences = [tokenize(sentence) for sentence in dissimilar_sentences]\n",
    "\n",
    "# print('similar_sentence tokens:', similar_sentences)\n",
    "# print('dissimilar_sentences tokens:', dissimilar_sentences)\n",
    "\n",
    "# Train a Word2Vec model on the sentences\n",
    "model = Word2Vec(similar_sentences + dissimilar_sentences, min_count=1)\n",
    "\n",
    "# Convert the sentences to vectors\n",
    "similar_vectors = [model.wv[sentence].mean(axis=0) for sentence in similar_sentences]\n",
    "dissimilar_vectors = [model.wv[sentence].mean(axis=0) for sentence in dissimilar_sentences]\n",
    "\n",
    "\n",
    "# print('similar_vectors:\\n', similar_vectors)\n",
    "# print('dissimilar_vectors:\\n', dissimilar_vectors)\n",
    "\n",
    "# Convert the vectors to PyTorch tensors\n",
    "similar_vectors = torch.tensor(np.array(similar_vectors)).float()\n",
    "dissimilar_vectors = torch.tensor(np.array(dissimilar_vectors)).float()\n",
    "\n",
    "# Calculate the loss for the similar and dissimilar sentences\n",
    "similar_loss = cosine_similarity_loss(similar_vectors[0].unsqueeze(0), similar_vectors[1].unsqueeze(0))\n",
    "dissimilar_loss = cosine_similarity_loss(dissimilar_vectors[0].unsqueeze(0), dissimilar_vectors[1].unsqueeze(0))\n",
    "\n",
    "\n",
    "# close to 0 == similar, closer to 1 == dissimilar\n",
    "print('Similar loss', similar_loss)\n",
    "print('Dissimilar loss', dissimilar_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
