{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding Models ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import corpora, models, similarities, downloader\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading word2vec model\n",
      "Downloading gigaword model\n",
      "Downloading fasttext model\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading word2vec model\")\n",
    "word2vec_model = gensim.downloader.load('word2vec-google-news-300')\n",
    "print(\"Downloading gigaword model\")\n",
    "glove_model = gensim.downloader.load('glove-wiki-gigaword-100')\n",
    "print(\"Downloading fasttext model\")\n",
    "fasttext_model = gensim.downloader.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordToVector(word, model):\n",
    "    try:\n",
    "        return model[word]\n",
    "    except KeyError:\n",
    "        # Handle out-of-vocabulary words\n",
    "        return np.zeros(model.vector_size)  # Return zero vector for OOV words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textToVectors(text, model):\n",
    "    tokens = tokenize(text)  # Tokenize the text\n",
    "    vectors = [wordToVector(token, model) for token in tokens]  # Convert words to vectors\n",
    "    vectors = np.array(vectors)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zachm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt') # needed this dependency to run this module\n",
    "\n",
    "word2vec_vectors = textToVectors(\"The quick brown fox jumped. Over the lazy, brown Dog!\", word2vec_model)\n",
    "glove_vectors = textToVectors(\"The quick brown fox jumped. Over the lazy, brown Dog!\", glove_model)\n",
    "fasttext_vectors = textToVectors(\"The quick brown fox jumped. Over the lazy, brown Dog!\", fasttext_model)\n",
    "\n",
    "# print(word2vec_vectors)\n",
    "# print(glove_vectors)\n",
    "# print(fasttext_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compressVectorsPCA(data, target_count):\n",
    "    pca = PCA(n_components=target_count)\n",
    "    compressed_data = pca.fit_transform(data.T).T\n",
    "    return compressed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 300)\n",
      "(7, 300)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(word2vec_vectors))\n",
    "if len(word2vec_vectors.shape) == 1:\n",
    "    word2vec_vectors = np.expand_dims(word2vec_vectors, axis=0)\n",
    "compressed_word2vec_vectors = compressVectorsPCA(word2vec_vectors, 7)\n",
    "print(np.shape(compressed_word2vec_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector1\n",
      " [[0.5998198  0.87476283 0.18173789 ... 0.34574115 0.46002654 0.4450597 ]\n",
      " [0.54420733 0.60855119 0.07550424 ... 0.24339172 0.49459274 0.45631048]\n",
      " [0.77527884 0.94949736 0.52133708 ... 0.40341107 0.61114772 0.42465905]\n",
      " ...\n",
      " [0.03183342 0.81069096 0.20642733 ... 0.43671026 0.04127853 0.97592815]\n",
      " [0.73900767 0.74957235 0.47429497 ... 0.79351917 0.5721325  0.60575341]\n",
      " [0.86258475 0.89757387 0.76513914 ... 0.75768511 0.75769022 0.74978716]]\n",
      "vector2\n",
      " [[0.19516028 0.02849873 0.92067956 ... 0.97349843 0.97812601 0.91455581]\n",
      " [0.9757859  0.96299027 0.28742315 ... 0.87991771 0.74952072 0.44609446]\n",
      " [0.14679533 0.79318475 0.08814172 ... 0.39968938 0.80172621 0.66361208]\n",
      " ...\n",
      " [0.45635654 0.12651347 0.37567982 ... 0.32325817 0.61019748 0.5223292 ]\n",
      " [0.08723689 0.79460083 0.76671261 ... 0.8334818  0.69967357 0.04181071]\n",
      " [0.24358481 0.36747512 0.88149076 ... 0.50479809 0.72011864 0.95091518]]\n",
      "loss tensor(0.0655)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "compressed_vector_1 = np.random.rand(7, 300)\n",
    "print('vector1\\n', compressed_vector_1)\n",
    "\n",
    "compressed_vector_2 = np.random.rand(7, 300)\n",
    "print('vector2\\n', compressed_vector_2)\n",
    "\n",
    "# Loss calculation\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "compressed_vector_1 = torch.from_numpy(compressed_vector_1).float()\n",
    "compressed_vector_2 = torch.from_numpy(compressed_vector_2).float()\n",
    "\n",
    "\n",
    "def cosine_similarity_loss(compressed_vectors_1, compressed_vectors_2):\n",
    "    # Normalize the compressed vectors\n",
    "    compressed_vectors_1 = F.normalize(compressed_vectors_1, dim=1)\n",
    "    compressed_vectors_2 = F.normalize(compressed_vectors_2, dim=1)\n",
    "    \n",
    "    # Calculate the cosine similarity between the compressed vectors\n",
    "    cosine_similarities = F.cosine_similarity(compressed_vectors_1, compressed_vectors_2)\n",
    "    \n",
    "    # Define the target labels (1 for similar pairs)\n",
    "    target = torch.ones_like(cosine_similarities)\n",
    "    \n",
    "    # Calculate the MSE loss\n",
    "    loss = F.mse_loss(cosine_similarities, target)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "loss = cosine_similarity_loss(compressed_vector_1, compressed_vector_2)\n",
    "print('loss', loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar loss tensor(0.1246)\n",
      "Dissimilar loss tensor(0.6277)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Define some similar and dissimilar sentences\n",
    "similar_sentences = [\"The cat sat on the mat\", \"A cat is sitting on the mat\"]\n",
    "dissimilar_sentences = [\"The cat sat on the mat\", \"Apple is a fruit\"]\n",
    "\n",
    "# Similar sentences are about the same sport (soccer)\n",
    "similar_sentences = [\"The soccer team won the match\", \"The football squad was victorious in the game\"]\n",
    "dissimilar_sentences = [\"The soccer team won the match\", \"The basketball player scored a three-pointer\"]\n",
    "\n",
    "# Similar sentences are about the same cooking action (baking a cake)\n",
    "similar_sentences = [\"She is baking a cake for the party\", \"A cake is being baked for the celebration\"]\n",
    "dissimilar_sentences = [\"She is baking a cake for the party\", \"He is frying an egg for breakfast\"]\n",
    "\n",
    "\n",
    "# Tokenize the sentences\n",
    "similar_sentences = [tokenize(sentence) for sentence in similar_sentences]\n",
    "dissimilar_sentences = [tokenize(sentence) for sentence in dissimilar_sentences]\n",
    "\n",
    "# print('similar_sentence tokens:', similar_sentences)\n",
    "# print('dissimilar_sentences tokens:', dissimilar_sentences)\n",
    "\n",
    "# Train a Word2Vec model on the sentences\n",
    "model = Word2Vec(similar_sentences + dissimilar_sentences, min_count=1)\n",
    "\n",
    "# Convert the sentences to vectors\n",
    "similar_vectors = [model.wv[sentence].mean(axis=0) for sentence in similar_sentences]\n",
    "dissimilar_vectors = [model.wv[sentence].mean(axis=0) for sentence in dissimilar_sentences]\n",
    "\n",
    "\n",
    "# print('similar_vectors:\\n', similar_vectors)\n",
    "# print('dissimilar_vectors:\\n', dissimilar_vectors)\n",
    "\n",
    "# Convert the vectors to PyTorch tensors\n",
    "similar_vectors = torch.tensor(np.array(similar_vectors)).float()\n",
    "dissimilar_vectors = torch.tensor(np.array(dissimilar_vectors)).float()\n",
    "\n",
    "# Calculate the loss for the similar and dissimilar sentences\n",
    "similar_loss = cosine_similarity_loss(similar_vectors[0].unsqueeze(0), similar_vectors[1].unsqueeze(0))\n",
    "dissimilar_loss = cosine_similarity_loss(dissimilar_vectors[0].unsqueeze(0), dissimilar_vectors[1].unsqueeze(0))\n",
    "\n",
    "\n",
    "# close to 0 == similar, closer to 1 == dissimilar\n",
    "print('Similar loss', similar_loss)\n",
    "print('Dissimilar loss', dissimilar_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
