{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Word Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zachm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import corpora, models, similarities, downloader\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading word2vec model\n",
      "Downloading gigaword model\n",
      "Downloading fasttext model\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading word2vec model\")\n",
    "word2vec_model = gensim.downloader.load('word2vec-google-news-300')\n",
    "print(\"Downloading gigaword model\")\n",
    "glove_model = gensim.downloader.load('glove-wiki-gigaword-100')\n",
    "print(\"Downloading fasttext model\")\n",
    "fasttext_model = gensim.downloader.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s\\d]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    # print(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordToVector(word, model):\n",
    "    try:\n",
    "        return model[word]\n",
    "    except KeyError:\n",
    "        # Handle out-of-vocabulary words\n",
    "        return np.zeros(model.vector_size)  # Return zero vector for OOV words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textToVectors(text, model):\n",
    "    tokens = tokenize(text)  # Tokenize the text\n",
    "    vectors = [wordToVector(token, model) for token in tokens]  # Convert words to vectors\n",
    "    vectors = np.array(vectors)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_vectors = textToVectors(\"The quick brown fox jumped. Over the lazy, brown Dog!\", word2vec_model)\n",
    "glove_vectors = textToVectors(\"The quick brown fox jumped. Over the lazy, brown Dog!\", glove_model)\n",
    "fasttext_vectors = textToVectors(\"The quick brown fox jumped. Over the lazy, brown Dog!\", fasttext_model)\n",
    "\n",
    "# print(word2vec_vectors)\n",
    "# print(glove_vectors)\n",
    "# print(fasttext_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Compression with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compressVectorsPCA(data, target_count):\n",
    "    pca = PCA(n_components=target_count)\n",
    "    compressed_data = pca.fit_transform(data.T).T\n",
    "    return compressed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 300)\n",
      "(7, 300)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(word2vec_vectors))\n",
    "if len(word2vec_vectors.shape) == 1:\n",
    "    word2vec_vectors = np.expand_dims(word2vec_vectors, axis=0)\n",
    "compressed_word2vec_vectors = compressVectorsPCA(word2vec_vectors, 7)\n",
    "print(np.shape(compressed_word2vec_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector To Best Match Word(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorToBestWords(target_vector, model, num_words=5):\n",
    "    # Check if model is a full Word2Vec model\n",
    "    if hasattr(model, 'wv'):\n",
    "        vectors = model.wv.vectors\n",
    "        index_to_key = model.wv.index_to_key\n",
    "    # If not, assume it's a KeyedVectors object\n",
    "    else:\n",
    "        vectors = model.vectors\n",
    "        index_to_key = model.index_to_key\n",
    "\n",
    "    # Compute cosine similarity between target vector and all word vectors\n",
    "    similarity_scores = np.dot(vectors, target_vector) / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(target_vector))\n",
    "\n",
    "    # Get indices of top N words with highest similarity scores\n",
    "    top_indices = similarity_scores.argsort()[-num_words:][::-1]\n",
    "\n",
    "    # Get top N words and their similarity scores\n",
    "    similar_words = [(index_to_key[idx], similarity_scores[idx]) for idx in top_indices]\n",
    "\n",
    "    return similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Vector -> Word using Glove\n",
      "Best matches for random vector:  [('db2', 0.40853849455847435), ('ldap', 0.38845534847767554), ('sqlite', 0.3835625956867372), ('zupljanin', 0.3681825436365551), ('eros', 0.365523298471245)]\n",
      "Best matches for specific vector for fast:  [('fast', 0.99999994), ('slow', 0.795973), ('faster', 0.75118226), ('pace', 0.7462931), ('speed', 0.71333927)]\n",
      "\n",
      "\n",
      "Performing Vector -> Word using Word2Vec\n",
      "Best matches for random vector:  [('week_begs##Jun##', 0.2413109026849108), ('Cindy_Crawford', 0.23725296150794603), ('Ovidiu_Rom', 0.2289217162930197), (\"l'_Affaire\", 0.22402579167550726), ('Sons_Grosset_&', 0.2196883822516193)]\n",
      "Best matches for specific vector for fast:  [('fast', 1.0000001), ('quick', 0.5701606), ('rapidly', 0.5525555), ('Fast', 0.5490224), ('quickly', 0.5393723)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Performing Vector -> Word using Glove\")\n",
    "target_vector_random = np.random.rand(100)  # Random target vector\n",
    "similar_words_random = vectorToBestWords(target_vector_random, glove_model, num_words=5)\n",
    "\n",
    "target_word = \"fast\"\n",
    "target_vector_word = wordToVector(target_word, glove_model)\n",
    "similar_words_word = vectorToBestWords(target_vector_word, glove_model, num_words=5)\n",
    "\n",
    "print(\"Best matches for random vector: \", similar_words_random)\n",
    "print(f\"Best matches for specific vector for {target_word}: \", similar_words_word)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"Performing Vector -> Word using Word2Vec\")\n",
    "target_vector_random = np.random.rand(300)  # Random target vector\n",
    "similar_words_random = vectorToBestWords(target_vector_random, word2vec_model, num_words=5)\n",
    "\n",
    "target_word = \"fast\"\n",
    "target_vector_word = wordToVector(target_word, word2vec_model)\n",
    "similar_words_word = vectorToBestWords(target_vector_word, word2vec_model, num_words=5)\n",
    "\n",
    "print(\"Best matches for random vector: \", similar_words_random)\n",
    "print(f\"Best matches for specific vector for {target_word}: \", similar_words_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector1\n",
      " [[0.37493081 0.89925593 0.68222099 ... 0.80354179 0.16098218 0.74905248]\n",
      " [0.57839804 0.10557084 0.05983191 ... 0.55696641 0.25766353 0.47368311]\n",
      " [0.90015595 0.96018257 0.92317255 ... 0.64130156 0.13815145 0.43976601]\n",
      " ...\n",
      " [0.81866496 0.29370606 0.31808073 ... 0.61089443 0.6236832  0.96462161]\n",
      " [0.85069772 0.52448952 0.61051311 ... 0.56898204 0.42140225 0.88865234]\n",
      " [0.32056013 0.96210239 0.39238596 ... 0.49111334 0.73328403 0.7347346 ]]\n",
      "vector2\n",
      " [[0.75036178 0.24471017 0.53446834 ... 0.84077725 0.90364866 0.83368172]\n",
      " [0.38934142 0.04549182 0.94027247 ... 0.83180536 0.56619628 0.39809632]\n",
      " [0.19763913 0.79561056 0.26689064 ... 0.2096431  0.7927108  0.91064082]\n",
      " ...\n",
      " [0.48262378 0.5227481  0.36765888 ... 0.57648586 0.57718368 0.47268441]\n",
      " [0.04774399 0.84220641 0.11906403 ... 0.34460865 0.00331085 0.50848442]\n",
      " [0.36156734 0.81143025 0.33937332 ... 0.41617547 0.51920126 0.70450542]]\n",
      "loss tensor(0.0626)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "compressed_vector_1 = np.random.rand(7, 300)\n",
    "print('vector1\\n', compressed_vector_1)\n",
    "\n",
    "compressed_vector_2 = np.random.rand(7, 300)\n",
    "print('vector2\\n', compressed_vector_2)\n",
    "\n",
    "# Loss calculation\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "compressed_vector_1 = torch.from_numpy(compressed_vector_1).float()\n",
    "compressed_vector_2 = torch.from_numpy(compressed_vector_2).float()\n",
    "\n",
    "\n",
    "def cosine_similarity_loss(compressed_vectors_1, compressed_vectors_2):\n",
    "    # Normalize the compressed vectors\n",
    "    compressed_vectors_1 = F.normalize(compressed_vectors_1, dim=1)\n",
    "    compressed_vectors_2 = F.normalize(compressed_vectors_2, dim=1)\n",
    "    \n",
    "    # Calculate the cosine similarity between the compressed vectors\n",
    "    cosine_similarities = F.cosine_similarity(compressed_vectors_1, compressed_vectors_2)\n",
    "    \n",
    "    # Define the target labels (1 for similar pairs)\n",
    "    target = torch.ones_like(cosine_similarities)\n",
    "    \n",
    "    # Calculate the MSE loss\n",
    "    loss = F.mse_loss(cosine_similarities, target)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "loss = cosine_similarity_loss(compressed_vector_1, compressed_vector_2)\n",
    "print('loss', loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar loss tensor(0.1246)\n",
      "Dissimilar loss tensor(0.6277)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Define some similar and dissimilar sentences\n",
    "similar_sentences = [\"The cat sat on the mat\", \"A cat is sitting on the mat\"]\n",
    "dissimilar_sentences = [\"The cat sat on the mat\", \"Apple is a fruit\"]\n",
    "\n",
    "# Similar sentences are about the same sport (soccer)\n",
    "similar_sentences = [\"The soccer team won the match\", \"The football squad was victorious in the game\"]\n",
    "dissimilar_sentences = [\"The soccer team won the match\", \"The basketball player scored a three-pointer\"]\n",
    "\n",
    "# Similar sentences are about the same cooking action (baking a cake)\n",
    "similar_sentences = [\"She is baking a cake for the party\", \"A cake is being baked for the celebration\"]\n",
    "dissimilar_sentences = [\"She is baking a cake for the party\", \"He is frying an egg for breakfast\"]\n",
    "\n",
    "\n",
    "# Tokenize the sentences\n",
    "similar_sentences = [tokenize(sentence) for sentence in similar_sentences]\n",
    "dissimilar_sentences = [tokenize(sentence) for sentence in dissimilar_sentences]\n",
    "\n",
    "# print('similar_sentence tokens:', similar_sentences)\n",
    "# print('dissimilar_sentences tokens:', dissimilar_sentences)\n",
    "\n",
    "# Train a Word2Vec model on the sentences\n",
    "model = Word2Vec(similar_sentences + dissimilar_sentences, min_count=1)\n",
    "\n",
    "# Convert the sentences to vectors\n",
    "similar_vectors = [model.wv[sentence].mean(axis=0) for sentence in similar_sentences]\n",
    "dissimilar_vectors = [model.wv[sentence].mean(axis=0) for sentence in dissimilar_sentences]\n",
    "\n",
    "\n",
    "# print('similar_vectors:\\n', similar_vectors)\n",
    "# print('dissimilar_vectors:\\n', dissimilar_vectors)\n",
    "\n",
    "# Convert the vectors to PyTorch tensors\n",
    "similar_vectors = torch.tensor(np.array(similar_vectors)).float()\n",
    "dissimilar_vectors = torch.tensor(np.array(dissimilar_vectors)).float()\n",
    "\n",
    "# Calculate the loss for the similar and dissimilar sentences\n",
    "similar_loss = cosine_similarity_loss(similar_vectors[0].unsqueeze(0), similar_vectors[1].unsqueeze(0))\n",
    "dissimilar_loss = cosine_similarity_loss(dissimilar_vectors[0].unsqueeze(0), dissimilar_vectors[1].unsqueeze(0))\n",
    "\n",
    "\n",
    "# close to 0 == similar, closer to 1 == dissimilar\n",
    "print('Similar loss', similar_loss)\n",
    "print('Dissimilar loss', dissimilar_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
