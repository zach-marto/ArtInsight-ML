{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Word Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zachm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import corpora, models, similarities, downloader\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading word2vec model\n",
      "Downloading gigaword model\n",
      "Downloading fasttext model\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading word2vec model\")\n",
    "word2vec_model = gensim.downloader.load('word2vec-google-news-300')\n",
    "print(\"Downloading gigaword model\")\n",
    "glove_model = gensim.downloader.load('glove-wiki-gigaword-100')\n",
    "print(\"Downloading fasttext model\")\n",
    "fasttext_model = gensim.downloader.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s\\d]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    # print(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordToVector(word, model):\n",
    "    try:\n",
    "        return np.array(model[word])\n",
    "    except KeyError:\n",
    "        # Handle out-of-vocabulary words\n",
    "        return np.zeros(model.vector_size)  # Return zero vector for OOV words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textToVectors(text, model):\n",
    "    tokens = tokenize(text)  # Tokenize the text\n",
    "    vectors = [wordToVector(token, model) for token in tokens]  # Convert words to vectors\n",
    "    vectors = np.array(vectors)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_vectors = textToVectors(\"The quick brown fox jumped. Over the lazy, brown Dog!\", word2vec_model)\n",
    "glove_vectors = textToVectors(\"The quick brown fox jumped. Over the lazy, brown Dog!\", glove_model)\n",
    "fasttext_vectors = textToVectors(\"The quick brown fox jumped. Over the lazy, brown Dog!\", fasttext_model)\n",
    "\n",
    "# print(word2vec_vectors)\n",
    "# print(glove_vectors)\n",
    "# print(fasttext_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Compression with PCA\n",
    "\n",
    "It seems like using PCA results in vectors losing any meaning to the word embedding models. Although this might technically preserve a lot of data, we would need a new model to interpret them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compressVectorsPCA(data, target_count):\n",
    "    pca = PCA(n_components=target_count)\n",
    "    compressed_data = pca.fit_transform(data.T).T\n",
    "    return compressed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 300)\n",
      "(7, 300)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(word2vec_vectors))\n",
    "if len(word2vec_vectors.shape) == 1:\n",
    "    word2vec_vectors = np.expand_dims(word2vec_vectors, axis=0)\n",
    "compressed_word2vec_vectors = compressVectorsPCA(word2vec_vectors, 7)\n",
    "print(np.shape(compressed_word2vec_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Compression With Averaging - UNFINISHED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "#Example usage:\n",
    "#[v1, v2, v3, v4], 2 -> [(v1 + v2) / 2, (v3 + v4) / 2]\n",
    "#[v1, v2, v3], 2 -> [(v1 + (v2/2)) / 1.5, ((v2/2) + v3) / 1.5]\n",
    "\n",
    "def compressVectorsWithAveraging(vectors, compression_size):\n",
    "    group_size = len(vectors) / compression_size\n",
    "    compressed_vectors = []\n",
    "    i = 0\n",
    "    while (i < len(vectors) - group_size):\n",
    "        # print(i)\n",
    "        group_end = i + group_size\n",
    "        # print(np.shape(vectors[0]))\n",
    "        new_vec = np.zeros_like(vectors[0])\n",
    "\n",
    "        while i < group_end:\n",
    "            new_vec = []\n",
    "            if (group_end - i) >= 1:\n",
    "                dist_to_prev_whole_number = (i % 1)\n",
    "                dist_to_next_whole_number = 1 - dist_to_prev_whole_number\n",
    "\n",
    "                new_vec += dist_to_next_whole_number * np.array(vectors[i])\n",
    "                new_vec += dist_to_prev_whole_number * np.array(vectors[i+1])\n",
    "                print(new_vec)\n",
    "                i += 1\n",
    "            else:\n",
    "                dist_to_prev_whole_number = (i % 1)\n",
    "                dist_to_next_whole_number = 1 - dist_to_prev_whole_number\n",
    "                dist_to_group_end = group_end - i\n",
    "\n",
    "        # print(np.shape(new_vec))\n",
    "        \n",
    "\n",
    "    return np.array(compressed_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.08007812  0.10498047  0.04980469  0.0534668  -0.06738281 -0.12060547\n",
      "  0.03515625 -0.11865234  0.04394531  0.03015137 -0.05688477 -0.07617188\n",
      "  0.01287842  0.04980469 -0.08496094 -0.06347656  0.00628662 -0.04321289\n",
      "  0.02026367  0.01330566 -0.01953125  0.09277344 -0.171875   -0.00131989\n",
      "  0.06542969  0.05834961 -0.08251953  0.0859375  -0.00318909  0.05859375\n",
      " -0.03491211 -0.0123291  -0.0480957  -0.00302124  0.05639648  0.01495361\n",
      " -0.07226562 -0.05224609  0.09667969  0.04296875 -0.03540039 -0.07324219\n",
      "  0.03271484 -0.06176758  0.00787354  0.0035553  -0.00878906  0.0390625\n",
      "  0.03833008  0.04443359  0.06982422  0.01263428 -0.00445557 -0.03320312\n",
      " -0.04272461  0.09765625 -0.02160645 -0.0378418   0.01190186 -0.01391602\n",
      " -0.11328125  0.09326172 -0.03930664 -0.11621094  0.02331543 -0.01599121\n",
      "  0.02636719  0.10742188 -0.00466919  0.09619141  0.0279541  -0.05395508\n",
      "  0.08544922 -0.03686523 -0.02026367 -0.08544922  0.125       0.14453125\n",
      "  0.0267334   0.15039062  0.05273438 -0.18652344  0.08154297 -0.01062012\n",
      " -0.03735352 -0.07324219 -0.07519531  0.03613281 -0.13183594  0.00616455\n",
      "  0.05078125  0.04516602  0.0100708  -0.15039062 -0.06005859  0.05761719\n",
      " -0.00692749  0.01586914 -0.0213623   0.10351562 -0.00029182 -0.046875\n",
      " -0.01635742 -0.07861328 -0.06933594  0.01635742 -0.03149414 -0.01373291\n",
      " -0.03662109 -0.08886719 -0.0480957  -0.01318359 -0.07177734  0.00588989\n",
      " -0.04614258  0.03979492  0.10058594 -0.04931641  0.07568359  0.03881836\n",
      " -0.16699219 -0.09619141 -0.10107422  0.02905273 -0.05786133 -0.01928711\n",
      " -0.04296875 -0.08398438 -0.01989746  0.05151367  0.00848389 -0.03613281\n",
      " -0.14941406 -0.01855469 -0.03637695 -0.07666016 -0.03955078 -0.06152344\n",
      " -0.02001953  0.04150391  0.03686523 -0.07226562  0.00592041 -0.06298828\n",
      "  0.00738525 -0.01586914  0.01611328 -0.01452637  0.00772095  0.10107422\n",
      " -0.00558472  0.01428223 -0.07617188  0.05639648 -0.01293945  0.03063965\n",
      " -0.02490234 -0.09863281  0.0324707  -0.02807617 -0.08105469  0.02062988\n",
      "  0.01611328 -0.04199219 -0.03491211 -0.03759766  0.05493164  0.01373291\n",
      "  0.02685547 -0.05859375 -0.07177734 -0.12011719 -0.02282715 -0.1640625\n",
      " -0.00361633 -0.05981445  0.07080078 -0.07714844  0.05175781 -0.04296875\n",
      " -0.04833984  0.0300293  -0.06591797 -0.03173828 -0.04882812 -0.03491211\n",
      "  0.05883789 -0.01464844  0.18066406  0.05688477  0.05249023  0.05786133\n",
      "  0.11669922  0.05200195 -0.0534668   0.01867676 -0.015625    0.00576782\n",
      " -0.07324219 -0.11621094  0.04052734  0.0625     -0.04321289  0.01055908\n",
      "  0.02172852  0.04248047  0.03271484  0.04418945  0.05761719  0.02612305\n",
      " -0.01831055 -0.02697754 -0.00674438  0.00509644 -0.11621094  0.00364685\n",
      "  0.05761719 -0.05957031 -0.08837891  0.0135498   0.04541016 -0.04638672\n",
      " -0.0177002  -0.0625      0.03442383 -0.02416992  0.03088379  0.09570312\n",
      "  0.07958984  0.03930664  0.0279541  -0.0859375   0.08105469  0.06640625\n",
      " -0.00041962 -0.06933594  0.03588867 -0.03417969  0.04492188 -0.00772095\n",
      " -0.00741577 -0.04760742  0.01397705 -0.09960938  0.0246582  -0.09960938\n",
      "  0.11474609  0.03173828  0.02209473  0.07226562  0.03686523  0.02563477\n",
      "  0.01367188 -0.02734375  0.00592041 -0.06738281  0.05053711 -0.02832031\n",
      " -0.04516602 -0.01733398  0.02111816  0.03515625 -0.04296875  0.06640625\n",
      "  0.12207031  0.12353516  0.0039978   0.04516602 -0.01855469  0.04833984\n",
      "  0.04516602  0.08691406  0.02941895  0.03759766  0.03442383 -0.07373047\n",
      " -0.0402832  -0.14648438 -0.02441406 -0.01953125  0.0065918  -0.0018158\n",
      " -0.01092529  0.09326172  0.06542969  0.01843262 -0.09326172 -0.01574707\n",
      " -0.07128906 -0.08935547 -0.07128906 -0.03015137 -0.01300049  0.01635742\n",
      " -0.01831055  0.01483154  0.00500488  0.00366211  0.04760742 -0.06884766]\n",
      "\n",
      "\n",
      "[ 0.04003906  0.05249023  0.02490234  0.0267334  -0.03369141 -0.06030273\n",
      "  0.01757812 -0.05932617  0.02197266  0.01507568 -0.02844238 -0.03808594\n",
      "  0.00643921  0.02490234 -0.04248047 -0.03173828  0.00314331 -0.02160645\n",
      "  0.01013184  0.00665283 -0.00976562  0.04638672 -0.0859375  -0.00065994\n",
      "  0.03271484  0.0291748  -0.04125977  0.04296875 -0.00159454  0.02929688\n",
      " -0.01745605 -0.00616455 -0.02404785 -0.00151062  0.02819824  0.00747681\n",
      " -0.03613281 -0.02612305  0.04833984  0.02148438 -0.0177002  -0.03662109\n",
      "  0.01635742 -0.03088379  0.00393677  0.00177765 -0.00439453  0.01953125\n",
      "  0.01916504  0.0222168   0.03491211  0.00631714 -0.00222778 -0.01660156\n",
      " -0.0213623   0.04882812 -0.01080322 -0.0189209   0.00595093 -0.00695801\n",
      " -0.05664062  0.04663086 -0.01965332 -0.05810547  0.01165771 -0.00799561\n",
      "  0.01318359  0.05371094 -0.00233459  0.0480957   0.01397705 -0.02697754\n",
      "  0.04272461 -0.01843262 -0.01013184 -0.04272461  0.0625      0.07226562\n",
      "  0.0133667   0.07519531  0.02636719 -0.09326172  0.04077148 -0.00531006\n",
      " -0.01867676 -0.03662109 -0.03759766  0.01806641 -0.06591797  0.00308228\n",
      "  0.02539062  0.02258301  0.0050354  -0.07519531 -0.0300293   0.02880859\n",
      " -0.00346375  0.00793457 -0.01068115  0.05175781 -0.00014591 -0.0234375\n",
      " -0.00817871 -0.03930664 -0.03466797  0.00817871 -0.01574707 -0.00686646\n",
      " -0.01831055 -0.04443359 -0.02404785 -0.0065918  -0.03588867  0.00294495\n",
      " -0.02307129  0.01989746  0.05029297 -0.0246582   0.0378418   0.01940918\n",
      " -0.08349609 -0.0480957  -0.05053711  0.01452637 -0.02893066 -0.00964355\n",
      " -0.02148438 -0.04199219 -0.00994873  0.02575684  0.00424194 -0.01806641\n",
      " -0.07470703 -0.00927734 -0.01818848 -0.03833008 -0.01977539 -0.03076172\n",
      " -0.01000977  0.02075195  0.01843262 -0.03613281  0.00296021 -0.03149414\n",
      "  0.00369263 -0.00793457  0.00805664 -0.00726318  0.00386047  0.05053711\n",
      " -0.00279236  0.00714111 -0.03808594  0.02819824 -0.00646973  0.01531982\n",
      " -0.01245117 -0.04931641  0.01623535 -0.01403809 -0.04052734  0.01031494\n",
      "  0.00805664 -0.02099609 -0.01745605 -0.01879883  0.02746582  0.00686646\n",
      "  0.01342773 -0.02929688 -0.03588867 -0.06005859 -0.01141357 -0.08203125\n",
      " -0.00180817 -0.02990723  0.03540039 -0.03857422  0.02587891 -0.02148438\n",
      " -0.02416992  0.01501465 -0.03295898 -0.01586914 -0.02441406 -0.01745605\n",
      "  0.02941895 -0.00732422  0.09033203  0.02844238  0.02624512  0.02893066\n",
      "  0.05834961  0.02600098 -0.0267334   0.00933838 -0.0078125   0.00288391\n",
      " -0.03662109 -0.05810547  0.02026367  0.03125    -0.02160645  0.00527954\n",
      "  0.01086426  0.02124023  0.01635742  0.02209473  0.02880859  0.01306152\n",
      " -0.00915527 -0.01348877 -0.00337219  0.00254822 -0.05810547  0.00182343\n",
      "  0.02880859 -0.02978516 -0.04418945  0.0067749   0.02270508 -0.02319336\n",
      " -0.0088501  -0.03125     0.01721191 -0.01208496  0.01544189  0.04785156\n",
      "  0.03979492  0.01965332  0.01397705 -0.04296875  0.04052734  0.03320312\n",
      " -0.00020981 -0.03466797  0.01794434 -0.01708984  0.02246094 -0.00386047\n",
      " -0.00370789 -0.02380371  0.00698853 -0.04980469  0.0123291  -0.04980469\n",
      "  0.05737305  0.01586914  0.01104736  0.03613281  0.01843262  0.01281738\n",
      "  0.00683594 -0.01367188  0.00296021 -0.03369141  0.02526855 -0.01416016\n",
      " -0.02258301 -0.00866699  0.01055908  0.01757812 -0.02148438  0.03320312\n",
      "  0.06103516  0.06176758  0.0019989   0.02258301 -0.00927734  0.02416992\n",
      "  0.02258301  0.04345703  0.01470947  0.01879883  0.01721191 -0.03686523\n",
      " -0.0201416  -0.07324219 -0.01220703 -0.00976562  0.0032959  -0.0009079\n",
      " -0.00546265  0.04663086  0.03271484  0.00921631 -0.04663086 -0.00787354\n",
      " -0.03564453 -0.04467773 -0.03564453 -0.01507568 -0.00650024  0.00817871\n",
      " -0.00915527  0.00741577  0.00250244  0.00183105  0.02380371 -0.03442383]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word2vec_vectors = textToVectors(\"The quick brown fox jumped. Over the lazy, brown Dog!\", word2vec_model)\n",
    "\n",
    "first_vec = word2vec_vectors[0]\n",
    "print(first_vec)\n",
    "print('\\n')\n",
    "print(first_vec/2)\n",
    "\n",
    "# for vec in word2vec_vectors:\n",
    "#     print(np.linalg.norm(vec))\n",
    "print()\n",
    "# compressed_vectors = compressVectorsWithAveraging(word2vec_vectors, 6)\n",
    "# for vec in compressed_vectors:\n",
    "#     print(np.linalg.norm(vec))\n",
    "\n",
    "# very_compressed_vectors = compressVectorsWithAveraging(word2vec_vectors, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best matches for compressed vector 0:  [('quick', 0.9999999), ('swift', 0.6208426), ('speedy', 0.5804499), ('fast', 0.57016057), ('easy', 0.56960756)]\n",
      "best matches for compressed vector 1:  [('fox', 0.99999994), ('foxes', 0.77625567), ('squirrel', 0.6794781), ('rabbit', 0.6482737), ('squirrels', 0.638612)]\n",
      "best matches for compressed vector 2:  [('over', 0.99999994), ('past', 0.5859714), ('Over', 0.5610154), ('overthe', 0.55483913), ('within', 0.4844896)]\n",
      "best matches for compressed vector 3:  [('the', 0.9999998), ('this', 0.5937378), ('in', 0.5429296), ('that', 0.526257), ('ofthe', 0.51502824)]\n",
      "best matches for compressed vector 4:  [('brown', 0.99999994), ('brownish', 0.70204836), ('reddish_brown', 0.69402885), ('reddish', 0.6582767), ('white', 0.65807706)]\n",
      "best matches for very compressed vector 0:  [('fox', 0.99999994), ('foxes', 0.77625567), ('squirrel', 0.6794781), ('rabbit', 0.6482737), ('squirrels', 0.638612)]\n",
      "best matches for very compressed vector 1:  [('the', 0.9999998), ('this', 0.5937378), ('in', 0.5429296), ('that', 0.526257), ('ofthe', 0.51502824)]\n"
     ]
    }
   ],
   "source": [
    "for i, vec in enumerate(compressed_vectors):\n",
    "    best_matches = vectorToBestWords(vec, word2vec_model, num_words=5)\n",
    "    print(f\"best matches for compressed vector {i}: \", best_matches)\n",
    "\n",
    "for i, vec in enumerate(very_compressed_vectors):\n",
    "    best_matches = vectorToBestWords(vec, word2vec_model, num_words=5)\n",
    "    print(f\"best matches for very compressed vector {i}: \", best_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector To Best Match Word(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorToBestWords(target_vector, model, num_words=5):\n",
    "    # Check if model is a full Word2Vec model\n",
    "    if hasattr(model, 'wv'):\n",
    "        vectors = model.wv.vectors\n",
    "        index_to_key = model.wv.index_to_key\n",
    "    # If not, assume it's a KeyedVectors object\n",
    "    else:\n",
    "        vectors = model.vectors\n",
    "        index_to_key = model.index_to_key\n",
    "\n",
    "    # Compute cosine similarity between target vector and all word vectors\n",
    "    similarity_scores = np.dot(vectors, target_vector) / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(target_vector))\n",
    "\n",
    "    # Get indices of top N words with highest similarity scores\n",
    "    top_indices = similarity_scores.argsort()[-num_words:][::-1]\n",
    "\n",
    "    # Get top N words and their similarity scores\n",
    "    similar_words = [(index_to_key[idx], similarity_scores[idx]) for idx in top_indices]\n",
    "\n",
    "    return similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Vector -> Word using Glove\n",
      "Best matches for random vector:  [('sunita', 0.3848831591768465), ('trnopolje', 0.36304550206512476), ('eros', 0.35788758373142765), ('omarska', 0.3487484460830099), ('mig-19', 0.34581193864475784)]\n",
      "Best matches for specific vector for fast:  [('fast', 0.99999994), ('slow', 0.795973), ('faster', 0.75118226), ('pace', 0.7462931), ('speed', 0.71333927)]\n",
      "\n",
      "\n",
      "Performing Vector -> Word using Word2Vec\n",
      "Best matches for random vector:  [('Legislative_Scorecard', 0.23590085064348204), ('AP_HOCKEY_NEWS', 0.23459775667067212), ('TRENDING_UP', 0.2237782728651971), ('Website_http://www.cgi.com', 0.2216507669298352), ('TRENDING_DOWN', 0.22140191310845694)]\n",
      "Best matches for specific vector for fast:  [('fast', 1.0000001), ('quick', 0.5701606), ('rapidly', 0.5525555), ('Fast', 0.5490224), ('quickly', 0.5393723)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Performing Vector -> Word using Glove\")\n",
    "target_vector_random = np.random.rand(100)  # Random target vector\n",
    "similar_words_random = vectorToBestWords(target_vector_random, glove_model, num_words=5)\n",
    "\n",
    "target_word = \"fast\"\n",
    "target_vector_word = wordToVector(target_word, glove_model)\n",
    "similar_words_word = vectorToBestWords(target_vector_word, glove_model, num_words=5)\n",
    "\n",
    "print(\"Best matches for random vector: \", similar_words_random)\n",
    "print(f\"Best matches for specific vector for {target_word}: \", similar_words_word)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"Performing Vector -> Word using Word2Vec\")\n",
    "target_vector_random = np.random.rand(300)  # Random target vector\n",
    "similar_words_random = vectorToBestWords(target_vector_random, word2vec_model, num_words=5)\n",
    "\n",
    "target_word = \"fast\"\n",
    "target_vector_word = wordToVector(target_word, word2vec_model)\n",
    "similar_words_word = vectorToBestWords(target_vector_word, word2vec_model, num_words=5)\n",
    "\n",
    "print(\"Best matches for random vector: \", similar_words_random)\n",
    "print(f\"Best matches for specific vector for {target_word}: \", similar_words_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateCosineAverage(vectors):\n",
    "    # Step 1: Compute the Cosine Average\n",
    "    cosine_average = np.mean(vectors, axis=0)\n",
    "\n",
    "    # Step 2: Normalize the Cosine Average\n",
    "    cosine_average_normalized = cosine_average / np.linalg.norm(cosine_average)\n",
    "\n",
    "    # Step 3: Scale by the Magnitude of an Average Embedding in Word2Vec\n",
    "    average_embedding_word2vec = np.mean(vectors, axis=0)  # average embedding from Word2Vec model\n",
    "    magnitude_average_embedding_word2vec = np.linalg.norm(average_embedding_word2vec)\n",
    "    final_embedding = cosine_average_normalized * magnitude_average_embedding_word2vec\n",
    "    \n",
    "    return final_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.36059600e-02  6.80725127e-02 -4.89990227e-02  1.12194821e-01\n",
      " -1.16943363e-02 -1.28198236e-01  5.59585616e-02 -8.33679140e-02\n",
      "  8.35205093e-02  1.18652344e-01  5.58349602e-02 -9.58496109e-02\n",
      "  6.34765625e-02 -1.51000973e-02 -8.40576142e-02 -7.44628906e-02\n",
      " -5.63110337e-02  1.16479486e-01 -7.74963349e-02 -8.32763687e-02\n",
      " -1.75170898e-02  5.23803718e-02  3.55957039e-02 -2.73242947e-02\n",
      "  1.38427736e-02  2.79712677e-02 -8.83911178e-02  1.39331058e-01\n",
      "  1.05416872e-01 -7.09228497e-03 -8.10546894e-03  4.92919907e-02\n",
      "  1.55029295e-03 -9.53308120e-02 -3.40087898e-02  1.55853275e-02\n",
      " -1.35498047e-02 -3.31665054e-02  8.46679658e-02  1.70312494e-01\n",
      "  7.78564438e-02 -1.68164060e-01  8.60687271e-02 -3.40576172e-02\n",
      "  4.49829102e-02 -6.75323457e-02 -1.45874023e-02 -5.28350845e-02\n",
      "  5.88500984e-02  8.78418013e-02 -4.73541245e-02  7.14965835e-02\n",
      "  9.27124023e-02  2.77130120e-02 -9.83886700e-03  9.34265107e-02\n",
      "  1.69677730e-03 -7.43896514e-02  3.88244614e-02 -3.19580063e-02\n",
      " -4.27551270e-02  1.29284665e-01 -1.23608395e-01 -1.24658205e-01\n",
      "  2.98645012e-02 -2.13867165e-02  1.79656986e-02  7.14111328e-03\n",
      " -5.37170433e-02  7.67517090e-02  1.30444333e-01  7.91015625e-02\n",
      "  1.66381840e-02 -6.37207041e-03 -1.33847043e-01  9.74731427e-03\n",
      "  5.56762703e-02  4.13574204e-02  5.29785175e-03  1.16931148e-01\n",
      "  3.00903320e-02 -6.71875030e-02  3.29467766e-02 -7.71118179e-02\n",
      " -2.68707294e-02 -4.49462906e-02 -9.07348618e-02  9.70458984e-03\n",
      "  6.22711182e-02 -9.32495147e-02 -6.75292984e-02  1.23400882e-01\n",
      " -1.11865237e-01 -8.51684585e-02 -2.15332024e-02 -4.77539077e-02\n",
      " -1.15173347e-02  3.56204994e-02 -3.37524409e-03 -1.42578129e-02\n",
      " -4.79351059e-02 -8.35449249e-02  7.75146484e-02 -3.16268913e-02\n",
      "  8.39843787e-03 -8.77197236e-02  1.00805663e-01 -2.66967751e-02\n",
      "  2.68188473e-02 -1.24153517e-01 -1.44042959e-02  8.54492173e-05\n",
      "  2.50244141e-02  5.46447709e-02  6.11450188e-02 -3.80371101e-02\n",
      "  6.44195527e-02 -6.63085952e-02  4.05273447e-03  7.82714859e-02\n",
      " -1.16503909e-01 -4.23583984e-02 -1.28857419e-01 -1.99584966e-03\n",
      " -3.97949219e-02  6.77764863e-02 -7.23144561e-02  4.10156231e-03\n",
      "  6.92626983e-02 -9.47265606e-03 -9.43481475e-02  1.58935543e-02\n",
      " -1.15124509e-01  4.30908203e-02 -4.60510254e-02  1.98059082e-02\n",
      "  5.78384399e-02 -1.19628906e-02  6.83425888e-02  1.62597641e-01\n",
      "  1.52587891e-03 -2.12036129e-02 -3.99658196e-02 -2.16064453e-02\n",
      "  2.95524602e-03  7.75878876e-02 -9.41040069e-02 -1.49566652e-02\n",
      "  1.59973148e-02 -6.71508759e-02  1.67510986e-01 -3.50707993e-02\n",
      " -1.73120111e-01  6.86462373e-02 -1.25927731e-01  6.85089082e-02\n",
      " -7.12585449e-03 -6.91391006e-02 -1.23046875e-01  6.47644028e-02\n",
      " -8.09082016e-02  2.31979378e-02  1.03210449e-01 -6.66503906e-02\n",
      " -3.54736336e-02 -8.46328773e-03  1.21758461e-01 -1.62841789e-02\n",
      "  1.01538092e-01 -4.01367173e-02 -9.86083969e-02  1.37939453e-02\n",
      " -4.93652336e-02  5.77026382e-02 -6.70547485e-02  1.99157707e-02\n",
      "  1.01538092e-01 -1.00228883e-01 -7.25097656e-02  9.97680649e-02\n",
      " -8.40454027e-02 -1.04840085e-01 -1.00854494e-01  9.60693322e-03\n",
      " -1.94900520e-02 -4.48303223e-02  1.87500007e-02  6.45172149e-02\n",
      "  5.50781265e-02  7.90008530e-02  3.71337906e-02 -1.15124509e-01\n",
      " -1.18896486e-02  7.17834458e-02 -1.78833003e-03  1.27624512e-01\n",
      " -4.67712358e-02 -5.77362068e-02 -7.07656890e-02  3.46679660e-03\n",
      "  4.50927727e-02 -2.60253903e-02 -6.93735853e-02 -2.98587792e-02\n",
      "  4.11499031e-02  2.29248051e-02  5.70800789e-02  2.63183601e-02\n",
      "  2.08511353e-02 -1.70562752e-02 -1.27495199e-01 -1.52099617e-02\n",
      " -5.80993667e-02  3.63464355e-02  1.65039059e-02  6.00799508e-02\n",
      "  8.96392837e-02 -1.34423822e-01 -1.37280270e-01 -3.94897461e-02\n",
      " -3.07861324e-02  5.73486313e-02 -6.31897002e-02 -7.38372803e-02\n",
      "  3.10668945e-02 -9.65332016e-02  1.56127930e-01 -2.98889168e-02\n",
      "  8.86047333e-02 -5.27832024e-02  1.39648438e-01 -8.97949263e-02\n",
      "  6.55517578e-02  3.23829651e-02 -3.78524773e-02  2.97851562e-02\n",
      "  1.59082025e-01 -5.23315445e-02  1.54504389e-01  8.21960419e-02\n",
      "  2.12384030e-01 -3.72070298e-02 -1.72729492e-02 -9.81689468e-02\n",
      "  1.33496091e-01  1.03515629e-02  5.98876998e-02  1.72607414e-02\n",
      "  5.95397986e-02 -1.46728521e-02 -8.81042518e-03 -1.92810055e-02\n",
      "  5.06835952e-02  1.16503909e-01  2.30712886e-03 -4.65515144e-02\n",
      " -7.17193633e-02  5.64880390e-03 -8.11523423e-02 -1.00708008e-02\n",
      "  1.23901367e-02  7.46704116e-02 -7.72949234e-02 -4.34570294e-03\n",
      "  7.94677734e-02  2.95410189e-03 -1.04815677e-01 -1.45323751e-02\n",
      " -7.37060532e-02  2.31506340e-02  5.56884818e-02  1.05371095e-01\n",
      " -2.83691399e-02  5.99121079e-02  8.96728486e-02 -6.61987290e-02\n",
      " -5.55034690e-02 -6.58203140e-02  3.52783198e-03  1.15007780e-01\n",
      "  9.51660126e-02  2.57919319e-02  6.32812530e-02  1.47808835e-01\n",
      " -2.86071766e-02 -5.06103523e-02 -5.08789048e-02  3.22875977e-02\n",
      " -1.48681644e-02 -2.53723152e-02 -3.88061516e-02  9.17968713e-03\n",
      " -8.90197679e-02  1.04846194e-01 -3.74023430e-02 -7.72827119e-02\n",
      "  7.69195566e-03  3.26507576e-02 -4.20227014e-02  5.99609427e-02]\n"
     ]
    }
   ],
   "source": [
    "word2vec_vectors = textToVectors(\"The quick brown fox jumped. Over the lazy, brown Dog!\", word2vec_model)\n",
    "cosine_avg = calculateCosineAverage(word2vec_vectors)\n",
    "print(cosine_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Vector -> Word2Vec Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector1\n",
      " [[7.39749257e-01 8.39965802e-02 7.03839622e-01 ... 5.73710887e-01\n",
      "  4.62328625e-01 3.18578680e-01]\n",
      " [8.22803691e-01 4.67929492e-01 1.76120936e-01 ... 9.44050707e-01\n",
      "  8.30725811e-01 1.29721382e-01]\n",
      " [9.82973136e-01 3.21305293e-01 7.44089749e-01 ... 2.67614653e-01\n",
      "  1.93074362e-01 5.24516552e-01]\n",
      " ...\n",
      " [7.77721563e-01 3.98477142e-01 4.73752422e-02 ... 6.38497226e-01\n",
      "  1.51927150e-02 3.23426927e-05]\n",
      " [1.79944801e-01 7.81132929e-03 3.40351758e-01 ... 2.26077714e-01\n",
      "  9.68746871e-01 1.14903095e-01]\n",
      " [9.02326391e-01 4.94349923e-01 7.32896543e-02 ... 2.86647824e-01\n",
      "  9.53655073e-01 9.08831145e-01]]\n",
      "vector2\n",
      " [[0.2669689  0.49207948 0.21668173 ... 0.3196332  0.24785025 0.96875742]\n",
      " [0.96472003 0.3580558  0.63984578 ... 0.02406716 0.82997334 0.57165175]\n",
      " [0.31159191 0.29564814 0.72713214 ... 0.74641691 0.39253504 0.84042203]\n",
      " ...\n",
      " [0.21530968 0.67735914 0.02367151 ... 0.72195895 0.58287576 0.98881385]\n",
      " [0.58858401 0.33128256 0.60276719 ... 0.69665234 0.28861399 0.9899224 ]\n",
      " [0.67471068 0.13198676 0.16792032 ... 0.97420318 0.06373409 0.44034061]]\n",
      "loss tensor(0.0735)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "compressed_vector_1 = np.random.rand(7, 300)\n",
    "print('vector1\\n', compressed_vector_1)\n",
    "\n",
    "compressed_vector_2 = np.random.rand(7, 300)\n",
    "print('vector2\\n', compressed_vector_2)\n",
    "\n",
    "# Loss calculation\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "compressed_vector_1 = torch.from_numpy(compressed_vector_1).float()\n",
    "compressed_vector_2 = torch.from_numpy(compressed_vector_2).float()\n",
    "\n",
    "\n",
    "def cosine_similarity_loss(compressed_vectors_1, compressed_vectors_2):\n",
    "    # Normalize the compressed vectors\n",
    "    compressed_vectors_1 = F.normalize(compressed_vectors_1, dim=1)\n",
    "    compressed_vectors_2 = F.normalize(compressed_vectors_2, dim=1)\n",
    "    \n",
    "    # Calculate the cosine similarity between the compressed vectors\n",
    "    cosine_similarities = F.cosine_similarity(compressed_vectors_1, compressed_vectors_2)\n",
    "    \n",
    "    # Define the target labels (1 for similar pairs)\n",
    "    target = torch.ones_like(cosine_similarities)\n",
    "    \n",
    "    # Calculate the MSE loss\n",
    "    loss = F.mse_loss(cosine_similarities, target)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "loss = cosine_similarity_loss(compressed_vector_1, compressed_vector_2)\n",
    "print('loss', loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar loss tensor(0.1246)\n",
      "Dissimilar loss tensor(0.6277)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Define some similar and dissimilar sentences\n",
    "similar_sentences = [\"The cat sat on the mat\", \"A cat is sitting on the mat\"]\n",
    "dissimilar_sentences = [\"The cat sat on the mat\", \"Apple is a fruit\"]\n",
    "\n",
    "# Similar sentences are about the same sport (soccer)\n",
    "similar_sentences = [\"The soccer team won the match\", \"The football squad was victorious in the game\"]\n",
    "dissimilar_sentences = [\"The soccer team won the match\", \"The basketball player scored a three-pointer\"]\n",
    "\n",
    "# Similar sentences are about the same cooking action (baking a cake)\n",
    "similar_sentences = [\"She is baking a cake for the party\", \"A cake is being baked for the celebration\"]\n",
    "dissimilar_sentences = [\"She is baking a cake for the party\", \"He is frying an egg for breakfast\"]\n",
    "\n",
    "\n",
    "# Tokenize the sentences\n",
    "similar_sentences = [tokenize(sentence) for sentence in similar_sentences]\n",
    "dissimilar_sentences = [tokenize(sentence) for sentence in dissimilar_sentences]\n",
    "\n",
    "# print('similar_sentence tokens:', similar_sentences)\n",
    "# print('dissimilar_sentences tokens:', dissimilar_sentences)\n",
    "\n",
    "# Train a Word2Vec model on the sentences\n",
    "model = Word2Vec(similar_sentences + dissimilar_sentences, min_count=1)\n",
    "\n",
    "# Convert the sentences to vectors\n",
    "similar_vectors = [model.wv[sentence].mean(axis=0) for sentence in similar_sentences]\n",
    "dissimilar_vectors = [model.wv[sentence].mean(axis=0) for sentence in dissimilar_sentences]\n",
    "\n",
    "\n",
    "# print('similar_vectors:\\n', similar_vectors)\n",
    "# print('dissimilar_vectors:\\n', dissimilar_vectors)\n",
    "\n",
    "# Convert the vectors to PyTorch tensors\n",
    "similar_vectors = torch.tensor(np.array(similar_vectors)).float()\n",
    "dissimilar_vectors = torch.tensor(np.array(dissimilar_vectors)).float()\n",
    "\n",
    "# Calculate the loss for the similar and dissimilar sentences\n",
    "similar_loss = cosine_similarity_loss(similar_vectors[0].unsqueeze(0), similar_vectors[1].unsqueeze(0))\n",
    "dissimilar_loss = cosine_similarity_loss(dissimilar_vectors[0].unsqueeze(0), dissimilar_vectors[1].unsqueeze(0))\n",
    "\n",
    "\n",
    "# close to 0 == similar, closer to 1 == dissimilar\n",
    "print('Similar loss', similar_loss)\n",
    "print('Dissimilar loss', dissimilar_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
