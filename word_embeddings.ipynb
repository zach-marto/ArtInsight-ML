{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Word Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zachm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import corpora, models, similarities, downloader\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading word2vec model\n",
      "Downloading gigaword model\n",
      "Downloading fasttext model\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading word2vec model\")\n",
    "word2vec_model = gensim.downloader.load('word2vec-google-news-300')\n",
    "print(\"Downloading gigaword model\")\n",
    "glove_model = gensim.downloader.load('glove-wiki-gigaword-100')\n",
    "print(\"Downloading fasttext model\")\n",
    "fasttext_model = gensim.downloader.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s\\d]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    # print(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordToVector(word, model):\n",
    "    try:\n",
    "        return model[word]\n",
    "    except KeyError:\n",
    "        # Handle out-of-vocabulary words\n",
    "        return np.zeros(model.vector_size)  # Return zero vector for OOV words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textToVectors(text, model):\n",
    "    tokens = tokenize(text)  # Tokenize the text\n",
    "    vectors = [wordToVector(token, model) for token in tokens]  # Convert words to vectors\n",
    "    vectors = np.array(vectors)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_vectors = textToVectors(\"The quick brown fox jumped. Over the lazy, brown Dog!\", word2vec_model)\n",
    "glove_vectors = textToVectors(\"The quick brown fox jumped. Over the lazy, brown Dog!\", glove_model)\n",
    "fasttext_vectors = textToVectors(\"The quick brown fox jumped. Over the lazy, brown Dog!\", fasttext_model)\n",
    "\n",
    "# print(word2vec_vectors)\n",
    "# print(glove_vectors)\n",
    "# print(fasttext_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Compression with PCA\n",
    "\n",
    "It seems like using PCA results in vectors losing any meaning to the word embedding models. Although this might technically preserve a lot of data, we would need a new model to interpret them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compressVectorsPCA(data, target_count):\n",
    "    pca = PCA(n_components=target_count)\n",
    "    compressed_data = pca.fit_transform(data.T).T\n",
    "    return compressed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 300)\n",
      "(7, 300)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(word2vec_vectors))\n",
    "if len(word2vec_vectors.shape) == 1:\n",
    "    word2vec_vectors = np.expand_dims(word2vec_vectors, axis=0)\n",
    "compressed_word2vec_vectors = compressVectorsPCA(word2vec_vectors, 7)\n",
    "print(np.shape(compressed_word2vec_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Compression With Averaging - UNFINISHED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "#Example usage:\n",
    "#[v1, v2, v3, v4], 2 -> [(v1 + v2) / 2, (v3 + v4) / 2]\n",
    "#[v1, v2, v3], 2 -> [(v1 + (v2/2)) / 1.5, ((v2/2) + v3) / 1.5]\n",
    "\n",
    "def compressVectorsWithAveraging(vectors, compression_size):\n",
    "    group_size = len(vectors) / compression_size\n",
    "    compressed_vectors = []\n",
    "    i = 0\n",
    "    while (i < len(vectors) - group_size):\n",
    "        # print(i)\n",
    "        group_end = i + group_size\n",
    "        # print(np.shape(vectors[0]))\n",
    "        new_vec = np.zeros_like(vectors[0])\n",
    "\n",
    "        while i < group_end:\n",
    "            new_vec = []\n",
    "            if (group_end - i) >= 1:\n",
    "                dist_to_prev_whole_number = (i % 1)\n",
    "                dist_to_next_whole_number = 1 - dist_to_prev_whole_number\n",
    "\n",
    "                new_vec = dist_to_next_whole_number * np.array(vectors[i])\n",
    "                print(new_vec)\n",
    "                i += 1\n",
    "\n",
    "        # print(np.shape(new_vec))\n",
    "        \n",
    "\n",
    "    return np.array(compressed_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ 0.08007812  0.10498047  0.04980469  0.0534668  -0.06738281 -0.12060547\n",
      "  0.03515625 -0.11865234  0.04394531  0.03015137 -0.05688477 -0.07617188\n",
      "  0.01287842  0.04980469 -0.08496094 -0.06347656  0.00628662 -0.04321289\n",
      "  0.02026367  0.01330566 -0.01953125  0.09277344 -0.171875   -0.00131989\n",
      "  0.06542969  0.05834961 -0.08251953  0.0859375  -0.00318909  0.05859375\n",
      " -0.03491211 -0.0123291  -0.0480957  -0.00302124  0.05639648  0.01495361\n",
      " -0.07226562 -0.05224609  0.09667969  0.04296875 -0.03540039 -0.07324219\n",
      "  0.03271484 -0.06176758  0.00787354  0.0035553  -0.00878906  0.0390625\n",
      "  0.03833008  0.04443359  0.06982422  0.01263428 -0.00445557 -0.03320312\n",
      " -0.04272461  0.09765625 -0.02160645 -0.0378418   0.01190186 -0.01391602\n",
      " -0.11328125  0.09326172 -0.03930664 -0.11621094  0.02331543 -0.01599121\n",
      "  0.02636719  0.10742188 -0.00466919  0.09619141  0.0279541  -0.05395508\n",
      "  0.08544922 -0.03686523 -0.02026367 -0.08544922  0.125       0.14453125\n",
      "  0.0267334   0.15039062  0.05273438 -0.18652344  0.08154297 -0.01062012\n",
      " -0.03735352 -0.07324219 -0.07519531  0.03613281 -0.13183594  0.00616455\n",
      "  0.05078125  0.04516602  0.0100708  -0.15039062 -0.06005859  0.05761719\n",
      " -0.00692749  0.01586914 -0.0213623   0.10351562 -0.00029182 -0.046875\n",
      " -0.01635742 -0.07861328 -0.06933594  0.01635742 -0.03149414 -0.01373291\n",
      " -0.03662109 -0.08886719 -0.0480957  -0.01318359 -0.07177734  0.00588989\n",
      " -0.04614258  0.03979492  0.10058594 -0.04931641  0.07568359  0.03881836\n",
      " -0.16699219 -0.09619141 -0.10107422  0.02905273 -0.05786133 -0.01928711\n",
      " -0.04296875 -0.08398438 -0.01989746  0.05151367  0.00848389 -0.03613281\n",
      " -0.14941406 -0.01855469 -0.03637695 -0.07666016 -0.03955078 -0.06152344\n",
      " -0.02001953  0.04150391  0.03686523 -0.07226562  0.00592041 -0.06298828\n",
      "  0.00738525 -0.01586914  0.01611328 -0.01452637  0.00772095  0.10107422\n",
      " -0.00558472  0.01428223 -0.07617188  0.05639648 -0.01293945  0.03063965\n",
      " -0.02490234 -0.09863281  0.0324707  -0.02807617 -0.08105469  0.02062988\n",
      "  0.01611328 -0.04199219 -0.03491211 -0.03759766  0.05493164  0.01373291\n",
      "  0.02685547 -0.05859375 -0.07177734 -0.12011719 -0.02282715 -0.1640625\n",
      " -0.00361633 -0.05981445  0.07080078 -0.07714844  0.05175781 -0.04296875\n",
      " -0.04833984  0.0300293  -0.06591797 -0.03173828 -0.04882812 -0.03491211\n",
      "  0.05883789 -0.01464844  0.18066406  0.05688477  0.05249023  0.05786133\n",
      "  0.11669922  0.05200195 -0.0534668   0.01867676 -0.015625    0.00576782\n",
      " -0.07324219 -0.11621094  0.04052734  0.0625     -0.04321289  0.01055908\n",
      "  0.02172852  0.04248047  0.03271484  0.04418945  0.05761719  0.02612305\n",
      " -0.01831055 -0.02697754 -0.00674438  0.00509644 -0.11621094  0.00364685\n",
      "  0.05761719 -0.05957031 -0.08837891  0.0135498   0.04541016 -0.04638672\n",
      " -0.0177002  -0.0625      0.03442383 -0.02416992  0.03088379  0.09570312\n",
      "  0.07958984  0.03930664  0.0279541  -0.0859375   0.08105469  0.06640625\n",
      " -0.00041962 -0.06933594  0.03588867 -0.03417969  0.04492188 -0.00772095\n",
      " -0.00741577 -0.04760742  0.01397705 -0.09960938  0.0246582  -0.09960938\n",
      "  0.11474609  0.03173828  0.02209473  0.07226562  0.03686523  0.02563477\n",
      "  0.01367188 -0.02734375  0.00592041 -0.06738281  0.05053711 -0.02832031\n",
      " -0.04516602 -0.01733398  0.02111816  0.03515625 -0.04296875  0.06640625\n",
      "  0.12207031  0.12353516  0.0039978   0.04516602 -0.01855469  0.04833984\n",
      "  0.04516602  0.08691406  0.02941895  0.03759766  0.03442383 -0.07373047\n",
      " -0.0402832  -0.14648438 -0.02441406 -0.01953125  0.0065918  -0.0018158\n",
      " -0.01092529  0.09326172  0.06542969  0.01843262 -0.09326172 -0.01574707\n",
      " -0.07128906 -0.08935547 -0.07128906 -0.03015137 -0.01300049  0.01635742\n",
      " -0.01831055  0.01483154  0.00500488  0.00366211  0.04760742 -0.06884766]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# for vec in word2vec_vectors:\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#     print(np.linalg.norm(vec))\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m----> 6\u001b[0m compressed_vectors \u001b[38;5;241m=\u001b[39m \u001b[43mcompressVectorsWithAveraging\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword2vec_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# for vec in compressed_vectors:\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#     print(np.linalg.norm(vec))\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# very_compressed_vectors = compressVectorsWithAveraging(word2vec_vectors, 3)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[74], line 17\u001b[0m, in \u001b[0;36mcompressVectorsWithAveraging\u001b[1;34m(vectors, compression_size)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# print(np.shape(vectors[0]))\u001b[39;00m\n\u001b[0;32m     15\u001b[0m new_vec \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(vectors[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m<\u001b[39m group_end:\n\u001b[0;32m     18\u001b[0m     new_vec \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (group_end \u001b[38;5;241m-\u001b[39m i) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "word2vec_vectors = textToVectors(\"The quick brown fox jumped. Over the lazy, brown Dog!\", word2vec_model)\n",
    "\n",
    "# for vec in word2vec_vectors:\n",
    "#     print(np.linalg.norm(vec))\n",
    "print()\n",
    "compressed_vectors = compressVectorsWithAveraging(word2vec_vectors, 6)\n",
    "# for vec in compressed_vectors:\n",
    "#     print(np.linalg.norm(vec))\n",
    "\n",
    "# very_compressed_vectors = compressVectorsWithAveraging(word2vec_vectors, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best matches for compressed vector 0:  [('quick', 0.9999999), ('swift', 0.6208426), ('speedy', 0.5804499), ('fast', 0.57016057), ('easy', 0.56960756)]\n",
      "best matches for compressed vector 1:  [('fox', 0.99999994), ('foxes', 0.77625567), ('squirrel', 0.6794781), ('rabbit', 0.6482737), ('squirrels', 0.638612)]\n",
      "best matches for compressed vector 2:  [('over', 0.99999994), ('past', 0.5859714), ('Over', 0.5610154), ('overthe', 0.55483913), ('within', 0.4844896)]\n",
      "best matches for compressed vector 3:  [('the', 0.9999998), ('this', 0.5937378), ('in', 0.5429296), ('that', 0.526257), ('ofthe', 0.51502824)]\n",
      "best matches for compressed vector 4:  [('brown', 0.99999994), ('brownish', 0.70204836), ('reddish_brown', 0.69402885), ('reddish', 0.6582767), ('white', 0.65807706)]\n",
      "best matches for very compressed vector 0:  [('fox', 0.99999994), ('foxes', 0.77625567), ('squirrel', 0.6794781), ('rabbit', 0.6482737), ('squirrels', 0.638612)]\n",
      "best matches for very compressed vector 1:  [('the', 0.9999998), ('this', 0.5937378), ('in', 0.5429296), ('that', 0.526257), ('ofthe', 0.51502824)]\n"
     ]
    }
   ],
   "source": [
    "for i, vec in enumerate(compressed_vectors):\n",
    "    best_matches = vectorToBestWords(vec, word2vec_model, num_words=5)\n",
    "    print(f\"best matches for compressed vector {i}: \", best_matches)\n",
    "\n",
    "for i, vec in enumerate(very_compressed_vectors):\n",
    "    best_matches = vectorToBestWords(vec, word2vec_model, num_words=5)\n",
    "    print(f\"best matches for very compressed vector {i}: \", best_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector To Best Match Word(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorToBestWords(target_vector, model, num_words=5):\n",
    "    # Check if model is a full Word2Vec model\n",
    "    if hasattr(model, 'wv'):\n",
    "        vectors = model.wv.vectors\n",
    "        index_to_key = model.wv.index_to_key\n",
    "    # If not, assume it's a KeyedVectors object\n",
    "    else:\n",
    "        vectors = model.vectors\n",
    "        index_to_key = model.index_to_key\n",
    "\n",
    "    # Compute cosine similarity between target vector and all word vectors\n",
    "    similarity_scores = np.dot(vectors, target_vector) / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(target_vector))\n",
    "\n",
    "    # Get indices of top N words with highest similarity scores\n",
    "    top_indices = similarity_scores.argsort()[-num_words:][::-1]\n",
    "\n",
    "    # Get top N words and their similarity scores\n",
    "    similar_words = [(index_to_key[idx], similarity_scores[idx]) for idx in top_indices]\n",
    "\n",
    "    return similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Vector -> Word using Glove\n",
      "Best matches for random vector:  [('sunita', 0.3848831591768465), ('trnopolje', 0.36304550206512476), ('eros', 0.35788758373142765), ('omarska', 0.3487484460830099), ('mig-19', 0.34581193864475784)]\n",
      "Best matches for specific vector for fast:  [('fast', 0.99999994), ('slow', 0.795973), ('faster', 0.75118226), ('pace', 0.7462931), ('speed', 0.71333927)]\n",
      "\n",
      "\n",
      "Performing Vector -> Word using Word2Vec\n",
      "Best matches for random vector:  [('Legislative_Scorecard', 0.23590085064348204), ('AP_HOCKEY_NEWS', 0.23459775667067212), ('TRENDING_UP', 0.2237782728651971), ('Website_http://www.cgi.com', 0.2216507669298352), ('TRENDING_DOWN', 0.22140191310845694)]\n",
      "Best matches for specific vector for fast:  [('fast', 1.0000001), ('quick', 0.5701606), ('rapidly', 0.5525555), ('Fast', 0.5490224), ('quickly', 0.5393723)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Performing Vector -> Word using Glove\")\n",
    "target_vector_random = np.random.rand(100)  # Random target vector\n",
    "similar_words_random = vectorToBestWords(target_vector_random, glove_model, num_words=5)\n",
    "\n",
    "target_word = \"fast\"\n",
    "target_vector_word = wordToVector(target_word, glove_model)\n",
    "similar_words_word = vectorToBestWords(target_vector_word, glove_model, num_words=5)\n",
    "\n",
    "print(\"Best matches for random vector: \", similar_words_random)\n",
    "print(f\"Best matches for specific vector for {target_word}: \", similar_words_word)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"Performing Vector -> Word using Word2Vec\")\n",
    "target_vector_random = np.random.rand(300)  # Random target vector\n",
    "similar_words_random = vectorToBestWords(target_vector_random, word2vec_model, num_words=5)\n",
    "\n",
    "target_word = \"fast\"\n",
    "target_vector_word = wordToVector(target_word, word2vec_model)\n",
    "similar_words_word = vectorToBestWords(target_vector_word, word2vec_model, num_words=5)\n",
    "\n",
    "print(\"Best matches for random vector: \", similar_words_random)\n",
    "print(f\"Best matches for specific vector for {target_word}: \", similar_words_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector1\n",
      " [[7.39749257e-01 8.39965802e-02 7.03839622e-01 ... 5.73710887e-01\n",
      "  4.62328625e-01 3.18578680e-01]\n",
      " [8.22803691e-01 4.67929492e-01 1.76120936e-01 ... 9.44050707e-01\n",
      "  8.30725811e-01 1.29721382e-01]\n",
      " [9.82973136e-01 3.21305293e-01 7.44089749e-01 ... 2.67614653e-01\n",
      "  1.93074362e-01 5.24516552e-01]\n",
      " ...\n",
      " [7.77721563e-01 3.98477142e-01 4.73752422e-02 ... 6.38497226e-01\n",
      "  1.51927150e-02 3.23426927e-05]\n",
      " [1.79944801e-01 7.81132929e-03 3.40351758e-01 ... 2.26077714e-01\n",
      "  9.68746871e-01 1.14903095e-01]\n",
      " [9.02326391e-01 4.94349923e-01 7.32896543e-02 ... 2.86647824e-01\n",
      "  9.53655073e-01 9.08831145e-01]]\n",
      "vector2\n",
      " [[0.2669689  0.49207948 0.21668173 ... 0.3196332  0.24785025 0.96875742]\n",
      " [0.96472003 0.3580558  0.63984578 ... 0.02406716 0.82997334 0.57165175]\n",
      " [0.31159191 0.29564814 0.72713214 ... 0.74641691 0.39253504 0.84042203]\n",
      " ...\n",
      " [0.21530968 0.67735914 0.02367151 ... 0.72195895 0.58287576 0.98881385]\n",
      " [0.58858401 0.33128256 0.60276719 ... 0.69665234 0.28861399 0.9899224 ]\n",
      " [0.67471068 0.13198676 0.16792032 ... 0.97420318 0.06373409 0.44034061]]\n",
      "loss tensor(0.0735)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "compressed_vector_1 = np.random.rand(7, 300)\n",
    "print('vector1\\n', compressed_vector_1)\n",
    "\n",
    "compressed_vector_2 = np.random.rand(7, 300)\n",
    "print('vector2\\n', compressed_vector_2)\n",
    "\n",
    "# Loss calculation\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "compressed_vector_1 = torch.from_numpy(compressed_vector_1).float()\n",
    "compressed_vector_2 = torch.from_numpy(compressed_vector_2).float()\n",
    "\n",
    "\n",
    "def cosine_similarity_loss(compressed_vectors_1, compressed_vectors_2):\n",
    "    # Normalize the compressed vectors\n",
    "    compressed_vectors_1 = F.normalize(compressed_vectors_1, dim=1)\n",
    "    compressed_vectors_2 = F.normalize(compressed_vectors_2, dim=1)\n",
    "    \n",
    "    # Calculate the cosine similarity between the compressed vectors\n",
    "    cosine_similarities = F.cosine_similarity(compressed_vectors_1, compressed_vectors_2)\n",
    "    \n",
    "    # Define the target labels (1 for similar pairs)\n",
    "    target = torch.ones_like(cosine_similarities)\n",
    "    \n",
    "    # Calculate the MSE loss\n",
    "    loss = F.mse_loss(cosine_similarities, target)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "loss = cosine_similarity_loss(compressed_vector_1, compressed_vector_2)\n",
    "print('loss', loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar loss tensor(0.1246)\n",
      "Dissimilar loss tensor(0.6277)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Define some similar and dissimilar sentences\n",
    "similar_sentences = [\"The cat sat on the mat\", \"A cat is sitting on the mat\"]\n",
    "dissimilar_sentences = [\"The cat sat on the mat\", \"Apple is a fruit\"]\n",
    "\n",
    "# Similar sentences are about the same sport (soccer)\n",
    "similar_sentences = [\"The soccer team won the match\", \"The football squad was victorious in the game\"]\n",
    "dissimilar_sentences = [\"The soccer team won the match\", \"The basketball player scored a three-pointer\"]\n",
    "\n",
    "# Similar sentences are about the same cooking action (baking a cake)\n",
    "similar_sentences = [\"She is baking a cake for the party\", \"A cake is being baked for the celebration\"]\n",
    "dissimilar_sentences = [\"She is baking a cake for the party\", \"He is frying an egg for breakfast\"]\n",
    "\n",
    "\n",
    "# Tokenize the sentences\n",
    "similar_sentences = [tokenize(sentence) for sentence in similar_sentences]\n",
    "dissimilar_sentences = [tokenize(sentence) for sentence in dissimilar_sentences]\n",
    "\n",
    "# print('similar_sentence tokens:', similar_sentences)\n",
    "# print('dissimilar_sentences tokens:', dissimilar_sentences)\n",
    "\n",
    "# Train a Word2Vec model on the sentences\n",
    "model = Word2Vec(similar_sentences + dissimilar_sentences, min_count=1)\n",
    "\n",
    "# Convert the sentences to vectors\n",
    "similar_vectors = [model.wv[sentence].mean(axis=0) for sentence in similar_sentences]\n",
    "dissimilar_vectors = [model.wv[sentence].mean(axis=0) for sentence in dissimilar_sentences]\n",
    "\n",
    "\n",
    "# print('similar_vectors:\\n', similar_vectors)\n",
    "# print('dissimilar_vectors:\\n', dissimilar_vectors)\n",
    "\n",
    "# Convert the vectors to PyTorch tensors\n",
    "similar_vectors = torch.tensor(np.array(similar_vectors)).float()\n",
    "dissimilar_vectors = torch.tensor(np.array(dissimilar_vectors)).float()\n",
    "\n",
    "# Calculate the loss for the similar and dissimilar sentences\n",
    "similar_loss = cosine_similarity_loss(similar_vectors[0].unsqueeze(0), similar_vectors[1].unsqueeze(0))\n",
    "dissimilar_loss = cosine_similarity_loss(dissimilar_vectors[0].unsqueeze(0), dissimilar_vectors[1].unsqueeze(0))\n",
    "\n",
    "\n",
    "# close to 0 == similar, closer to 1 == dissimilar\n",
    "print('Similar loss', similar_loss)\n",
    "print('Dissimilar loss', dissimilar_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
