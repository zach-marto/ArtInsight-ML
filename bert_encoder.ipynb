{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Necessary Dependencies and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_bert_model():\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_id_creater(tokenizer, sentence):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', 'using', 'bert', '.']\n"
     ]
    }
   ],
   "source": [
    "exp_tokenizer, exp_model = pretrained_bert_model()\n",
    "\n",
    "exp_text = \"Here is the sentence I want embeddings for using BERT.\"\n",
    "exp_tokens = exp_tokenizer.tokenize(exp_text)\n",
    "\n",
    "print(exp_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2182, 2003, 1996, 6251, 1045, 2215, 7861, 8270, 4667, 2015, 2005, 2478, 14324, 1012]\n",
      "tensor([[ 2182,  2003,  1996,  6251,  1045,  2215,  7861,  8270,  4667,  2015,\n",
      "          2005,  2478, 14324,  1012]])\n"
     ]
    }
   ],
   "source": [
    "exp_tokens_ids = exp_tokenizer.convert_tokens_to_ids(exp_tokens)\n",
    "\n",
    "print(exp_tokens_ids)\n",
    "print(torch.tensor([exp_tokens_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 14, 768])\n",
      "torch.Size([1, 14, 768])\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "exp_output = exp_model(torch.tensor([exp_tokens_ids]))\n",
    "\n",
    "# print(exp_output of final hidden state)\n",
    "print(exp_output[0].shape)\n",
    "\n",
    "# alternatively, we can also use last_hidden_state attribute.\n",
    "print(exp_output.last_hidden_state.shape)\n",
    "\n",
    "print(exp_output[1].shape)\n",
    "\n",
    "# still figuring out how to access the embeddings from the hidden layers of BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### experimenting with passing two sentences into bert at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 14, 768])\n"
     ]
    }
   ],
   "source": [
    "exp_text_2 = \"Here is a sentence I want embeddings for using BERT.\"\n",
    "exp_tokens_2 = exp_tokenizer.tokenize(exp_text_2)\n",
    "\n",
    "exp_tokens_ids_2 = exp_tokenizer.convert_tokens_to_ids(exp_tokens_2)\n",
    "\n",
    "exp_output_2 = exp_model(torch.tensor([exp_tokens_ids, exp_tokens_ids_2]))\n",
    "\n",
    "print(exp_output_2[0].shape)\n",
    "\n",
    "# passing multiple sentences to BERT model will return the embeddings for each sentence separately unaffected by the other sentences.\n",
    "# if we want to get the embeddings for the entire text, we need to pass the entire text as a single sentence - concatenate the sentences or tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating embeddings for individual words in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vocabulary import vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mvocab\u001b[49m))\n\u001b[1;32m      3\u001b[0m examp \u001b[38;5;241m=\u001b[39m vocab[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(exp_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(examp))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(vocab))\n",
    "\n",
    "examp = vocab[1]\n",
    "print(exp_tokenizer.tokenize(examp))\n",
    "examp_token_id = token_id_creater(exp_tokenizer, examp)\n",
    "print(examp_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 words\n",
      "Processed 2000 words\n",
      "Processed 3000 words\n",
      "Processed 4000 words\n",
      "Processed 5000 words\n",
      "Processed 6000 words\n",
      "Processed 7000 words\n",
      "Processed 8000 words\n",
      "Processed 9000 words\n",
      "Processed 10000 words\n",
      "Processed 11000 words\n",
      "Processed 12000 words\n",
      "Processed 13000 words\n",
      "Processed 14000 words\n",
      "Processed 15000 words\n",
      "Processed 16000 words\n",
      "Processed 17000 words\n",
      "Processed 18000 words\n",
      "Processed 19000 words\n",
      "Processed 20000 words\n",
      "Processed 21000 words\n",
      "Processed 22000 words\n",
      "Processed 23000 words\n",
      "Processed 24000 words\n",
      "Processed 25000 words\n",
      "Processed 26000 words\n",
      "Processed 27000 words\n",
      "Processed 28000 words\n",
      "Processed 29000 words\n",
      "Processed 30000 words\n",
      "Processed 31000 words\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "vocabulary_embeddings = {}\n",
    "\n",
    "i = 0\n",
    "\n",
    "# creating an example vocab of only the first 10000 words\n",
    "exp_vocab = vocab\n",
    "\n",
    "for word in exp_vocab:\n",
    "\n",
    "    i += 1\n",
    "    \n",
    "    token_id_curr = token_id_creater(exp_tokenizer, word)\n",
    "    output = exp_model(torch.tensor([token_id_curr]))\n",
    "\n",
    "    # storing the pooled output\n",
    "    vocabulary_embeddings[word] = output[1]\n",
    "\n",
    "    if (i % 1000 == 0):\n",
    "        print(f\"Processed {i} words\")\n",
    "\n",
    "\n",
    "# print number of key-value pairs in vocabulary_embeddings\n",
    "print(len(vocabulary_embeddings.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary_embeddings['air'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(input_embedding1, input_embedding2):\n",
    "    \n",
    "    # Normalize the input and reconstructed embeddings\n",
    "    input_embeddings1_norm = F.normalize(input_embedding1, p=2, dim=-1)\n",
    "    input_embedding2_norm = F.normalize(input_embedding2, p=2, dim=-1)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_similarities = torch.sum(input_embeddings1_norm * input_embedding2_norm, dim=-1)\n",
    "\n",
    "    # return cosine similarity\n",
    "    return cosine_similarities.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.701630711555481\n"
     ]
    }
   ],
   "source": [
    "# testing cosine similarity function\n",
    "\n",
    "print(cosine_similarity(vocabulary_embeddings['air'], vocabulary_embeddings['aah']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# now testing it for embeddings of vectors that are similar\n",
    "\n",
    "embedding1 = exp_model(torch.tensor([token_id_creater(exp_tokenizer, 'air')]))\n",
    "\n",
    "print(cosine_similarity(embedding1[1], vocabulary_embeddings['air']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now using it to create a function that returns the most similar word from a vector as input\n",
    "\n",
    "def most_similar_word(input_embedding, vocabulary_embeddings):\n",
    "    most_similar_word = None\n",
    "    max_similarity = -1\n",
    "\n",
    "    for word, embedding in vocabulary_embeddings.items():\n",
    "        similarity = cosine_similarity(input_embedding, embedding)\n",
    "        if similarity > max_similarity:\n",
    "            most_similar_word = word\n",
    "            max_similarity = similarity\n",
    "\n",
    "    return most_similar_word, max_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('air', 1.0)\n"
     ]
    }
   ],
   "source": [
    "# testing most_similar_word function\n",
    "\n",
    "print(most_similar_word(embedding1[1], vocabulary_embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 14, 768])\n",
      "14\n",
      "[('censure', 0.09438993781805038), ('clad', 0.0928436815738678), ('censure', 0.13513685762882233), ('beam', 0.1256704330444336), ('beam', 0.12083977460861206), ('airing', 0.07985206693410873), ('combo', 0.08452193439006805), ('combo', 0.0784529447555542), ('attract', 0.09313622862100601), ('beam', 0.10810612887144089), ('chabuks', 0.0981726348400116), ('credent', 0.06288355588912964), ('censure', 0.06640621274709702), ('bays', 0.08555959165096283)]\n"
     ]
    }
   ],
   "source": [
    "print(exp_output[0].shape)\n",
    "\n",
    "# iterate through all embeddings in the output and store their closest words in a \n",
    "\n",
    "closest_words = []\n",
    "\n",
    "print(exp_output[0].shape[1])\n",
    "\n",
    "for i in range(exp_output[0].shape[1]):\n",
    "    word_embedding = exp_output[0][:, i, :]\n",
    "    word, similarity = most_similar_word(word_embedding, vocabulary_embeddings)\n",
    "    closest_words.append((word, similarity))\n",
    "\n",
    "print(closest_words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
